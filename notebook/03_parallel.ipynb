{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57dab9c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parallel Programming\n",
    "\n",
    "**Author:** Nico Curti\n",
    "\n",
    "**Course:** Software and Computing for Applied Physics - 87948\n",
    "\n",
    "**Github:** [Nico-Curti](https://github.com/Nico-Curti)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec91375",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The starting point is always:\n",
    "    \n",
    "    “Premature optimization is the root of all evil” (cit. Donald Knuth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e90d1c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And, moreover:\n",
    "\n",
    "    “Not all the tasks can be performed in parallel! A code parallelization must be your last choice since it is certainly \n",
    "    the harderst way in which optimize a code!” (cit. Nico Curti)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce291fd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The idea\n",
    "\n",
    "The idea appears quite simple and it is the same of every best-practice approach to a problem:\n",
    "\n",
    "* divide the problem into a series of simpler tasks\n",
    "* give an order to these tasks\n",
    "* if they are **independent** there are no reason to perform them one-by-one into a sequential mode, so you can (try to) parallelize them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd13a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Languages and tools like `Make`, `CMake`, and `Snakemake` are based on this idea and they provide easy solution to address the parallelism.\n",
    "\n",
    "$$\\uparrow$$ \n",
    "\n",
    "    if you are not well confident about your programming skills they are certainly the best approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff02e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The approach is quite close also to the *Map-Reduce* paradigm of the Functional programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a817a6a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "a = np.arange(10)\n",
    "for i in range(10):\n",
    "    a[i] += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0abd905",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "All these tasks are totally independent, so they can be potentially performed in parallel!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d981d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallelism in Python\n",
    "\n",
    "Python provides a series of libraries dedicated to the parallel programming without any external dependency.\n",
    "\n",
    "The most famous one is [multiprocessing](https://docs.python.org/3/library/multiprocessing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ad4d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**NOTE:** \n",
    "\n",
    "The use of this library has several issues in the IPython console and under Windows OS it requires the inclusion of a \"main\" \n",
    "\n",
    "```python\n",
    "def func (x):\n",
    "    return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    y = func(2)\n",
    "    print(y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36247ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**NOTE 2**:\n",
    "\n",
    "If you want to use this library into an IPython console (like me, using the current Jupyter notebook!) you can easily move to the `multiprocess` counter part, which is a simple wrap of the standard library.\n",
    "\n",
    "Jupyter notebooks don't work with `multiprocessing` because the module pickles (serialises) data to send to processes.\n",
    "`multiprocess` is a fork of multiprocessing that uses dill instead of pickle to serialise data which allows it to work from within Jupyter notebooks. \n",
    "The API is identical!\n",
    "\n",
    "```bash\n",
    "python -m pip install multiprocess\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c912bf9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.022000 sec\n",
      "[ 0  2  4  6  8 10 12 14 16 18]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time as now\n",
    "\n",
    "N = 100_000\n",
    "a = np.arange(N)\n",
    "tic = now()\n",
    "\n",
    "for i in range(N):\n",
    "    a[i] *= 2\n",
    "toc = now()\n",
    "print(f'Elapsed time: {toc - tic:.6f} sec')\n",
    "print(a[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bde6ed5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using 24 threads in parallel\n",
      "Elapsed time: 2.664999 sec\n",
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n"
     ]
    }
   ],
   "source": [
    "import multiprocess\n",
    "\n",
    "N = 100_000\n",
    "a = np.arange(N)\n",
    "\n",
    "# get the number of possible threads\n",
    "nth = multiprocess.cpu_count()\n",
    "print(f'We are using {nth:d} threads in parallel')\n",
    "\n",
    "tic = now()\n",
    "with multiprocess.Pool(nth) as pool:\n",
    "    a = pool.map(lambda x : x*2, a)\n",
    "toc = now()\n",
    "\n",
    "print(f'Elapsed time: {toc - tic:.6f} sec')\n",
    "print(a[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163fa194",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Not a so good result...\n",
    "\n",
    "Using 24 parallel processes we have a more than 100x slower code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ac48b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why?\n",
    "\n",
    "* The creation of multiple parallel processes is a very expensive task in terms of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ae09b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If there are other processes in backend (very common situation in a personal computer!) the split of the job is not so easy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fee813",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Despite the great number of iterations, the task is not particularly intense from a computational point of view, so we are loosing more time in the job orchestration than in the computational task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e0916",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Last but not least, the multiprocessing in Python is a sort of fake..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404545e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's try with a more complex example\n",
    "\n",
    "We will use one old friend to this purpose..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8529bf9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi with 10000 steps for 1000 runs is 3.1409284 in 8.518996000289917 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 10000 # number of MC events\n",
    "N_run = 1000 # number of runs\n",
    "pi = np.zeros(N_run) # values of pi\n",
    "tic = now()\n",
    "for I in range(N_run):\n",
    "    Nhits = 0.0\n",
    "    for i in range(N):\n",
    "        x = np.random.rand() * 2 - 1\n",
    "        y = np.random.rand() * 2 - 1\n",
    "        distance = x*x + y*y\n",
    "        if distance < 1:\n",
    "            Nhits += 1.0\n",
    "    pi[I] = 4. * Nhits / N\n",
    "        \n",
    "toc = now()\n",
    "print('pi with', N, 'steps for', N_run, \n",
    "      'runs is', np.mean(pi), 'in', toc-tic, 'sec'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a10af61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the Metropolis algorithm, all the evaluation of the different PI computation are totally independent, so it could be a good candidate for a parallel approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a108d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's move the computation into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f39f063b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def Metropolis (I):\n",
    "    Nhits = 0.0\n",
    "    N = 10000 # number of MC events\n",
    "    for i in range(N):\n",
    "        x = np.random.rand() * 2 - 1\n",
    "        y = np.random.rand() * 2 - 1\n",
    "        distance = x*x + y*y\n",
    "        if distance < 1:\n",
    "            Nhits += 1.0\n",
    "    return 4. * Nhits / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f455cb01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can split the different evaluation of the Metropolis estimation into a series of independent tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9963b99e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\utente\\AppData\\Roaming\\Python\\Python310\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\utente\\AppData\\Roaming\\Python\\Python310\\site-packages\\multiprocess\\pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"C:\\Users\\utente\\AppData\\Local\\Temp\\ipykernel_32116\\425835812.py\", line 5, in Metropolis\nNameError: name 'np' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m tic \u001b[38;5;241m=\u001b[39m now()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m multiprocess\u001b[38;5;241m.\u001b[39mPool(nth) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m----> 3\u001b[0m     pi \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMetropolis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mN_run\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m toc \u001b[38;5;241m=\u001b[39m now()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpi with\u001b[39m\u001b[38;5;124m'\u001b[39m, N, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteps for\u001b[39m\u001b[38;5;124m'\u001b[39m, N_run, \n\u001b[0;32m      6\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns is\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(pi), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m'\u001b[39m, toc\u001b[38;5;241m-\u001b[39mtic, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msec\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m      )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\multiprocess\\pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "tic = now()\n",
    "with multiprocess.Pool(nth) as pool:\n",
    "    pi = pool.map(Metropolis, range(N_run))\n",
    "toc = now()\n",
    "print('pi with', N, 'steps for', N_run, \n",
    "      'runs is', np.mean(pi), 'in', toc-tic, 'sec'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7c466",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WTF?!\n",
    "\n",
    "I have already imported `numpy` package!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec83c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The \"process\" created by the library is a sort of multiple runs of \"independent\" scripts, like\n",
    "\n",
    "```bash\n",
    "$ python run_iteration1.py\n",
    "$ python run_iteration2.py\n",
    "$ ...\n",
    "$ python run_iterationN.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e88bee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A possible workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86591988",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def Metropolis (I):\n",
    "    import numpy as np\n",
    "    \n",
    "    Nhits = 0.0\n",
    "    N = 10000 # number of MC events\n",
    "    for i in range(N):\n",
    "        x = np.random.rand() * 2 - 1\n",
    "        y = np.random.rand() * 2 - 1\n",
    "        distance = x*x + y*y\n",
    "        if distance < 1:\n",
    "            Nhits += 1.0\n",
    "    return 4. * Nhits / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c456c8ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using 24 threads in parallel\n",
      "pi with 10000 steps for 1000 runs is 3.1419528 in 1.2859978675842285 sec\n"
     ]
    }
   ],
   "source": [
    "tic = now()\n",
    "with multiprocess.Pool(nth) as pool:\n",
    "    pi = pool.map(Metropolis, range(N_run))\n",
    "toc = now()\n",
    "print(f'We are using {nth:d} threads in parallel')\n",
    "print('pi with', N, 'steps for', N_run, \n",
    "      'runs is', np.mean(pi), 'in', toc-tic, 'sec'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88adf1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some **NOTE**:\n",
    "\n",
    "* The sequential computation of the Metropolis took 8.5 sec\n",
    "* The paralle computation of the Metropolis took 1.3 sec\n",
    "* The global speed up is **6.5x**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582282b7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We have obtained a 6.5x gain using 24x computational power... not so good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3d2dc9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Change the loop\n",
    "\n",
    "In our algorithm there are 2 nested loops... what happens moving the parallelization into the inner loop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee2d3d2c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using 24 threads in parallel\n",
      "pi with 10000 steps for 100 runs is 3.1452480000000005 in 48.4399356842041 sec\n"
     ]
    }
   ],
   "source": [
    "N = 10000 # number of MC events\n",
    "N_run = 100 # number of runs\n",
    "pi = np.zeros(N_run) # values of pi\n",
    "\n",
    "def innerMetropolis (i):\n",
    "    import numpy as np\n",
    "    x = np.random.rand() * 2 - 1\n",
    "    y = np.random.rand() * 2 - 1\n",
    "    distance = x*x + y*y\n",
    "    return distance < 1\n",
    "\n",
    "tic = now()\n",
    "for I in range(N_run):\n",
    "    with multiprocess.Pool(nth) as pool:\n",
    "        Nhits = sum(pool.map(innerMetropolis, range(N)))\n",
    "    pi[I] = 4. * Nhits / N\n",
    "    \n",
    "toc = now()\n",
    "\n",
    "print(f'We are using {nth:d} threads in parallel')\n",
    "print('pi with', N, 'steps for', N_run, \n",
    "      'runs is', np.mean(pi), 'in', toc-tic, 'sec'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed0dab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main rule of code optimization\n",
    "\n",
    "To get the best performances of your code:\n",
    "\n",
    "1. Parallelize the outer loop\n",
    "2. Vectorize the inner loop(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a10dff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another example\n",
    "\n",
    "Let's try another old friend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5920fcb7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.63 s ± 34.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def point_distance (N):\n",
    "    pt = np.random.uniform(0, 1, size=(N, 2))    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            x1, y1 = pt[i]\n",
    "            x2, y2 = pt[j]\n",
    "            d = euclidean([x1, y1], [x2, y2])\n",
    "\n",
    "%timeit point_distance(N=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c75fad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimize it with parallel programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7be95b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def point_distance (i, pt):\n",
    "    import numpy as np\n",
    "    from scipy.spatial.distance import euclidean\n",
    "    \n",
    "    for j in range(len(pt)):\n",
    "        x1, y1 = pt[i]\n",
    "        x2, y2 = pt[j]\n",
    "        d = euclidean([x1, y1], [x2, y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b955bc4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.120 sec\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "pt = np.random.uniform(0, 1, size=(N, 2))    \n",
    "\n",
    "tic = now()\n",
    "with multiprocess.Pool(nth) as pool:\n",
    "    for i in range(N):\n",
    "        pool.apply_async(point_distance, args=(i, pt))\n",
    "toc = now()\n",
    "print(f'Elapsed time: {toc - tic:.3f} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc3456",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can we do it better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f4bec4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Try to optimize the inner loop with the vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a230b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Try to balance the work of different threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e244cb1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Try to **increase** the work of the threads!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "415c8496",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "list(product(range(3), range(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64d66d08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def point_distance (pi, pj):\n",
    "    from scipy.spatial.distance import euclidean\n",
    "    d = euclidean(pi, pj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca1054bd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 21.882 sec\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "pt = np.random.uniform(0, 1, size=(N, 2))    \n",
    "\n",
    "tic = now()\n",
    "with multiprocess.Pool(nth) as pool:\n",
    "    for (i, j) in product(range(N), range(N)):\n",
    "        pool.apply_async(point_distance, args=(pt[i], pt[j]))\n",
    "toc = now()\n",
    "print(f'Elapsed time: {toc - tic:.3f} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f707b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This sounds counterintuitive!\n",
    "\n",
    "The cost for the creation of new threads in Python is **very** expensive and the increment of parallelism doesn't provide a real gain with high-level languages!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c6b1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallelism in low-level\n",
    "\n",
    "OpenMP is an Application Program Interface (API) that provides a portable, scalable model for developers of shared memory parallel applications. \n",
    "OpenMP works with Symmetric Multiprocessing (SMP) The API supports C/C++ and Fortran on a wide variety of architectures.\n",
    "\n",
    "[Official Guide](https://www.openmp.org/resources/refguides/)\n",
    "\n",
    "[Simple Guide](https://curc.readthedocs.io/en/latest/programming/OpenMP-C.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280c8e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The parallelism is given by the insertion of pre-processor directives identified by the syntax:\n",
    "\n",
    "```cpp\n",
    "#include <omp.h>\n",
    "\n",
    "#pragma omp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2660b906",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard program\n",
    "\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include <omp.h>\n",
    "\n",
    "int main(){\n",
    "    std :: cout << \"Hello from process: \" \n",
    "                << omp_get_thread_num() \n",
    "                << std :: endl;\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa8c55",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "$ g++ hello_world.cpp -o hello_world -fopenmp\n",
    "$ ./hello_world\n",
    "Hello from process: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0764ce36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel program\n",
    "\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include <omp.h>\n",
    "\n",
    "int main(){\n",
    "    #pragma omp parallel\n",
    "    {\n",
    "        std :: cout << \"Hello from process: \" \n",
    "                    << omp_get_thread_num() \n",
    "                    << std :: endl;\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55220777",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "$ g++ parallel_hello_world.cpp -o parallel_hello_world -fopenmp\n",
    "$ ./parallel_hello_world\n",
    "Hello from process: 3\n",
    "Hello from process: 0\n",
    "Hello from process: 2\n",
    "Hello from process: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81109bae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory Management\n",
    "\n",
    "Memory management is a quintessential component of any parallel program that involves data manipulation.\n",
    "\n",
    "* **Private** types create a copy of a variable for each process in the parallel system.\n",
    "* **Shared** types hold one instance of a variable for all processes to share."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a14210",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```cpp\n",
    "#pragma omp shared(shar_Var1) private(priv_Var1, priv_Var2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d62e6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```cpp\n",
    "#include <iostream>\n",
    "#include <omp.h>\n",
    "\n",
    "int main(){\n",
    "    int tid;\n",
    "    #pragma omp parallel\n",
    "    {\n",
    "        tid = omp_get_thread_num();\n",
    "        std :: cout << \"Hello from process: \" \n",
    "                    << tid\n",
    "                    << std :: endl;\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a243c2ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "Hello from process: 3\n",
    "Hello from process: 2\n",
    "Hello from process: 2\n",
    "Hello from process: 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab87f00e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```cpp\n",
    "#include <iostream>\n",
    "#include <omp.h>\n",
    "\n",
    "int main(){\n",
    "    int tid;\n",
    "    #pragma omp parallel private(tid)\n",
    "    {\n",
    "        tid = omp_get_thread_num();\n",
    "        std :: cout << \"Hello from process: \" \n",
    "                    << tid\n",
    "                    << std :: endl;\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d2b06",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "Hello from process: 3\n",
    "Hello from process: 0\n",
    "Hello from process: 2\n",
    "Hello from process: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b7609",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Concurrency\n",
    "\n",
    "Strictly related to the memory management \"problems\" there is the idea of concurrency.\n",
    "\n",
    "Since the variables are shared (as default) between the processes, the possibility to \"touch\" the same variables at the same time is not negligible, causing a *concurrency issue*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000bf7df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The management of several workers requires the introduction of an **orchestrator** who poses *barriers* and *critial directives*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1bc0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```cpp\n",
    "#include <omp.h>\n",
    "\n",
    "int main () {\n",
    "    int partial_Sum, total_Sum;\n",
    "\n",
    "    #pragma omp parallel private(partial_Sum) shared(total_Sum)\n",
    "    {\n",
    "        partial_Sum = 0;\n",
    "        total_Sum = 0;\n",
    "\n",
    "        #pragma omp for\n",
    "        {\n",
    "            for(int i = 1; i < 1000; ++i){\n",
    "                partial_Sum += i;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ce605",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each thread initializes its own copy of the `partial_Sum` variable with a value of zero.\n",
    "\n",
    "Then, inside a parallel loop (each thread takes a part of the 1000 possible iterations!), each thread increments its own variable.\n",
    "\n",
    "To compute the total sum we need to put a \"critical barrier\" at the end, saying:\n",
    "\n",
    "    In this part can enter a thread one-by-one (Safe region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69269ba8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```cpp\n",
    "#include <omp.h>\n",
    "\n",
    "int main () {\n",
    "    int partial_Sum, total_Sum;\n",
    "\n",
    "    #pragma omp parallel private(partial_Sum) shared(total_Sum)\n",
    "    {\n",
    "        partial_Sum = 0;\n",
    "        total_Sum = 0;\n",
    "\n",
    "        #pragma omp for\n",
    "        {\n",
    "            for(int i = 1; i < 1000; ++i){\n",
    "                partial_Sum += i;\n",
    "            }\n",
    "        }\n",
    "        //Create thread safe region.\n",
    "        #pragma omp critical\n",
    "        {\n",
    "            //add each threads partial sum to the total sum\n",
    "            total_Sum += partial_Sum;\n",
    "        }\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7643950b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another possibility in this cases is given by the **reduction** clause.\n",
    "\n",
    "The idea is always the same but, since the *Map-Reduce* paradigm is quite standard, OpenMP provides a ready-to-use feature for these cases.\n",
    "\n",
    "`\n",
    "reduction (op : list)\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d16cc08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```cpp\n",
    "#include <omp.h>\n",
    "\n",
    "int main () {\n",
    "    int total_Sum = 0;\n",
    "\n",
    "    #pragma omp parallel for reduction (+ : total_Sum)\n",
    "    for (int i = 0; i < 1000; ++i) {\n",
    "        total_Sum += i;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e11e26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "Parallel Metropolis Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9312d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```cpp\n",
    "#include <random>\n",
    "#include <iostream>\n",
    "#include <numeric>\n",
    "#include <iomanip>\n",
    "#include <omp.h>\n",
    "#define NUM_THREADS omp_get_max_threads()\n",
    "\n",
    "int main()\n",
    "{\n",
    "\tstd :: random_device rd;\n",
    "\tstd :: mt19937 eng{rd()};\n",
    "\tstd :: size_t i, I, N = 1e5, N_run = 1e4, Nhits = 0;\n",
    "\tlong double x, y, res, pi = 0.0, pi_tmp = 0.0;\n",
    "    \n",
    "\tstd :: uniform_real_distribution < long double > uniform_dist{-1., 1.};\n",
    "    \n",
    "    omp_set_num_threads(NUM_THREADS);\n",
    "\t\n",
    "\tlong double start_time = omp_get_wtime();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391cdf0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```cpp\n",
    "    \n",
    "    #pragma omp parallel private (I, Nhits)\n",
    "\t{\n",
    "\t\t#pragma omp for reduction (+ : pi_tmp)\n",
    "\t\tfor (I = 0; I < N_run; ++I)\n",
    "\t\t{\n",
    "\t\t\tNhits = 0;\n",
    "\t\t\t#pragma omp parallel private (i, x, y, res)\n",
    "\t\t\t{\n",
    "\t\t\t\t#pragma omp for reduction (+ : Nhits)\n",
    "\t\t\t\tfor (i = 0; i < N; ++i)\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\tx = uniform_dist(eng);\n",
    "\t\t\t\t\ty = uniform_dist(eng);\n",
    "\t\t\t\t\tres = x*x + y*y;\n",
    "\t\t\t\t\tif (res < 1)\n",
    "\t\t\t\t\t\tNhits += 1;\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t\tpi_tmp += (4.0 * ((long double)Nhits/(long double)N));\n",
    "\t\t}\n",
    "\t}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a688eed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```cpp\n",
    "\n",
    "    pi = pi_tmp / N_run;\n",
    "\tlong double run_time = omp_get_wtime() - start_time;\n",
    "\tstd :: cout << \"pi with \" << N << \" steps for \" << N_run << \" runs is \" << std :: set_precision(16) << pi << \" in \" << run_time << \" sec\" << std :: endl;\n",
    "\n",
    "\treturn 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb70f17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The same idea can be applied also considering different computer as \"workers\", moving to a *multithreading* paradigm to a *distributed* paradigm.\n",
    "\n",
    "Formally this can be done moving from *OpenMP* to *MPI* directives (with some edits and features!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896886eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recursive Approach\n",
    "\n",
    "There are several algorithms that can be better represented as recursive functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9fb6e393",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fibonacci sequence:\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "8\n",
      "13\n",
      "21\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    else:\n",
    "        return(fibonacci(n-1) + fibonacci(n-2))\n",
    "\n",
    "nterms = 10\n",
    "print('Fibonacci sequence:')\n",
    "for i in range(nterms):\n",
    "    print(fibonacci(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e39062b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The possibility to re-call the same function can cause infinite loops if not carefully managed but it could be also the most elegant way to represent a tree scheme of computing.\n",
    "\n",
    "The same approach is used also by **Metaprogramming** paradigm: if you are interested on this topic a very interesting example is given in the [PYTHON-easyDag](https://github.com/eDIMESLab/easyDAG) developed by Prof. Giampieri (and in its c++ counter part of my project [CPP-easyDag](https://github.com/Nico-Curti/easyDAG))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec313232",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The paradigm is always the same:\n",
    "\n",
    "1. split the job into a series of sub-jobs\n",
    "2. repeat the step 1 until a stop criteria\n",
    "3. perform the computation\n",
    "4. merge together the intermediate results until the root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f37ee1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Is it possible to work with multiple threads in this scheme?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55a1096",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As for the `omp parallel for` directive, there is also the possibility to manage the threads according to independent `task` and `section`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48840738",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```cpp\n",
    "#pragma omp parallel sections\n",
    "{\n",
    "    #pragma omp section\n",
    "    {\n",
    "        Task 1\n",
    "    }\n",
    "    #pragma omp section\n",
    "    {\n",
    "        Task 2\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a098e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "OpenMP sections are **static**, that is, they are good if you know, when you are writing the program, how many of them you will need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8f3c6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```cpp\n",
    "omp_set_num_threads ( 2 );\n",
    "#pragma omp parallel default (none)\n",
    "{\n",
    "    #pragma omp task\n",
    "    fprintf( stderr, \"A\\n\" );\n",
    "    \n",
    "    #pragma omp task\n",
    "    fprintf( stderr, \"B\\n\" );\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4719048a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tasks are very much like OpenMP **sections**, but sections are static, that is, the number of sections is set when you write the code, whereas **tasks** can be created anytime, and in any number, under control of your program's logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade5429",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Merge-sort example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974924af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start from the Python version to better clarify the idea behind the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0c7310fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def mergeSort(arr):\n",
    "    if len(arr)<=1:\n",
    "        return arr\n",
    "    mid = len(arr) // 2\n",
    "    left = mergeSort(arr[:mid])\n",
    "    right = mergeSort(arr[mid:])\n",
    "    \n",
    "    return merge(left, right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cdef292a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def merge(arr1, arr2):\n",
    "    i, j = 0, 0\n",
    "    result = []\n",
    "    while (i < len(arr1) and j < len(arr2)):\n",
    "        if arr2[j] > arr1[i]:\n",
    "            result.append(arr1[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(arr2[j])\n",
    "            j += 1\n",
    "    while (i < len(arr1)):\n",
    "        result.append(arr1[i])\n",
    "        i += 1\n",
    "    while (j < len(arr2)):\n",
    "        result.append(arr2[j])\n",
    "        j += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d308f775",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given array is:   [12, 11, 13, 5, 6, 7]\n",
      "Sorted array is:  [5, 6, 7, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "arr = [12, 11, 13, 5, 6, 7]\n",
    "print('Given array is:  ', arr)\n",
    "arr = mergeSort(arr)\n",
    "print('Sorted array is: ', arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66993c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Move to a more advanced implementation\n",
    "\n",
    "The idea is to split the work among all the possible threads, creating a tree of instruction.\n",
    "\n",
    "Each thread will perform the analysis of a small chunk of the array, producing the sorting of its slice in **serial** mode (!).\n",
    "\n",
    "The result of the entire array will be merged together, going back along the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfefb1b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```cpp\n",
    "void mergesort_serial (float * a, int start, int end)\n",
    "{\n",
    "  if ((end - start) == 2) {\n",
    "    if (a[start] < a[end - 1]) {\n",
    "      return;\n",
    "    } else {\n",
    "      std :: swap(a[start], a[end - 1]);\n",
    "      return;\n",
    "    }\n",
    "  }\n",
    "  const int pivot = start + ((end-start) >> 1);\n",
    "\n",
    "  if ((end - start) < 100) { // minimum sorting size\n",
    "    std :: sort(a + start, a + end);\n",
    "    return;\n",
    "  }  else  {\n",
    "    mergesort_serial(a, start, pivot, ord);\n",
    "    mergesort_serial(a, pivot, end, ord);\n",
    "  }\n",
    "  std :: inplace_merge(a + start, a + pivot, a + end, ord);\n",
    "\n",
    "  return;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165e2150",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```cpp\n",
    "void mergesort_parallel_omp (float * a, int start, int end, int threads)\n",
    "{\n",
    "  const int pivot = start + ((end - start) >> 1);\n",
    "\n",
    "  if (threads <= 1) {\n",
    "    mergesort_serial(a, start, end);\n",
    "    return;\n",
    "  } else {\n",
    "#pragma omp task shared (start, end, threads)\n",
    "    {\n",
    "      mergesort_parallel_omp(a, start, pivot, threads >> 1);\n",
    "    }\n",
    "#pragma omp task shared (start, end, threads)\n",
    "    {\n",
    "      mergesort_parallel_omp(a, pivot, end, threads - (threads >> 1));\n",
    "    }\n",
    "#pragma omp taskwait\n",
    "  }\n",
    "\n",
    "  std :: inplace_merge(a + start, a + pivot, a + end);\n",
    "\n",
    "  return;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a436d38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```cpp\n",
    "void sort (float * a, int start, int end)\n",
    "{\n",
    "  const int ths = omp_get_num_threads();\n",
    "  const int nth = (ths % 2) ? ths - 1 : ths;\n",
    "  const int diff = end % nth;\n",
    "  const int size = diff ? end - diff : end;\n",
    "\n",
    "  #pragma omp single\n",
    "  #pragma omp taskgroup\n",
    "  {\n",
    "    mergesort_parallel_omp(a, start, size, nth);\n",
    "  } // end single section\n",
    "\n",
    "  if (diff)\n",
    "  {\n",
    "    std :: sort(a + size, a + end);\n",
    "    std :: inplace_merge(a + start, a + size, a + end);\n",
    "  }\n",
    "\n",
    "  return;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f4af4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other possibility of parallelism\n",
    "\n",
    "There is also another possibility to split your code-work among different processes, called **asynchronous programming**.\n",
    "\n",
    "This is probably the most common approach in standard programming and one of the most powerful (and sometimes hardest) one.\n",
    "\n",
    "We will see some examples about it in the next application."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
